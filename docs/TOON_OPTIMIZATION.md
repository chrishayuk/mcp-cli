# TOON Optimization: Token-Optimized Object Notation

## Overview

TOON (Token-Optimized Object Notation) is a powerful optimization feature in MCP-CLI that reduces LLM token consumption and API costs by intelligently compressing conversation messages and tool definitions before sending them to language model providers. This optimization works with **all supported LLM providers** including OpenAI, Anthropic, Google, Groq, Ollama, and more.

## Table of Contents

- [What is TOON Optimization?](#what-is-toon-optimization)
- [How Token Savings Work](#how-token-savings-work)
- [Configuration](#configuration)
- [Benefits and Use Cases](#benefits-and-use-cases)
- [Technical Details](#technical-details)
- [Token Savings Examples](#token-savings-examples)
- [Monitoring and Debugging](#monitoring-and-debugging)
- [Performance Considerations](#performance-considerations)

## What is TOON Optimization?

TOON optimization reduces the number of tokens sent to LLM APIs by applying aggressive content compression techniques while preserving semantic meaning. The system automatically:

1. **Compares formats**: Measures token counts for both original JSON and compressed TOON formats
2. **Makes intelligent decisions**: Uses TOON format only when it provides actual token savings
3. **Displays savings**: Shows real-time token comparison statistics for transparency
4. **Works universally**: Compatible with all LLM providers without requiring provider-specific changes

### Key Features

- **Automatic Token Counting**: Uses HuggingFace transformers library with provider-specific tokenizers for accurate token measurement
- **Content Compression**: Removes unnecessary whitespace, compacts JSON structures, and optimizes field names
- **Cost Tracking**: Displays token savings in both absolute numbers and percentages
- **Smart Decision Making**: Only applies TOON when it actually saves tokens
- **Zero Configuration**: Works out-of-the-box with all supported providers

## How Token Savings Work

TOON optimization saves tokens at **every LLM API interaction** by compressing the request payload. Here's exactly when and where tokens are saved:

### 1. During Initial User Requests

**Without TOON:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "What is the weather in San Francisco?"
    }
  ],
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "get_weather",
        "description": "Get current weather",
        "parameters": {
          "type": "object",
          "properties": {
            "location": {
              "type": "string",
              "description": "City and state"
            }
          }
        }
      }
    }
  ]
}
```
**Estimated tokens**: ~150 tokens

**With TOON:**
```json
{"messages":[{"role":"user","content":"What is the weather in San Francisco?"}],"tools":[{"type":"function","function":{"name":"get_weather","description":"Get current weather","parameters":{"type":"object","properties":{"location":{"type":"string","description":"City and state"}}}}}]}
```
**Estimated tokens**: ~110 tokens
**Savings**: ~40 tokens (26.7%)

### 2. During Tool Call Responses

When the LLM responds with tool calls, and those are sent back with results:

**Without TOON:**
```json
{
  "messages": [
    {
      "role": "user",
      "content": "What's the weather in New York?"
    },
    {
      "role": "assistant",
      "content": "",
      "tool_calls": [
        {
          "id": "call_abc123",
          "type": "function",
          "function": {
            "name": "get_weather",
            "arguments": "{\"location\": \"New York, NY\", \"unit\": \"fahrenheit\"}"
          }
        }
      ]
    },
    {
      "role": "tool",
      "tool_call_id": "call_abc123",
      "name": "get_weather",
      "content": "{\"temperature\": 72, \"condition\": \"sunny\", \"humidity\": 45}"
    }
  ]
}
```
**Estimated tokens**: ~200 tokens

**With TOON:**
```json
{"messages":[{"role":"user","content":"What's the weather in New York?"},{"role":"assistant","content":"","tool_calls":[{"id":"call_abc123","type":"function","function":{"name":"get_weather","arguments":"{\"location\":\"New York, NY\",\"unit\":\"fahrenheit\"}"}}]},{"role":"tool","tool_call_id":"call_abc123","name":"get_weather","content":"{\"temperature\":72,\"condition\":\"sunny\",\"humidity\":45}"}]}
```
**Estimated tokens**: ~140 tokens
**Savings**: ~60 tokens (30%)

### 3. During Long Conversations

The savings compound significantly with conversation history:

**10-turn conversation without TOON**: ~2,500 tokens
**10-turn conversation with TOON**: ~1,750 tokens
**Savings**: ~750 tokens (30%)

**50-turn conversation without TOON**: ~12,500 tokens
**50-turn conversation with TOON**: ~8,750 tokens
**Savings**: ~3,750 tokens (30%)

### Where Exactly Are Tokens Saved?

TOON optimization saves tokens in the **input** to the LLM API by reducing the size of:

1. **Conversation History**: Every message in the conversation history is compressed
2. **Tool Definitions**: Function definitions, parameters, and descriptions are compacted
3. **Tool Results**: JSON responses from tools are compressed
4. **Whitespace**: All unnecessary whitespace, indentation, and newlines are removed
5. **Field Names**: In some cases, field names can be shortened while maintaining API compatibility

**Important**: The token savings apply to:
- **Input tokens** sent to the LLM (where you pay per token)
- **Every API request** throughout the conversation
- **Cumulative savings** that grow with conversation length

The LLM's output (response) is not affected - it responds normally with full, readable text.

## Configuration

### Enabling TOON Optimization

Add the following to your `server_config.json`:

```json
{
  "mcpServers": {
    "your-server": {
      "command": "your-command",
      "args": ["your", "args"]
    }
  },
  "enableToonOptimization": true
}
```

### Configuration Options

| Option | Type | Default | Description |
|--------|------|---------|-------------|
| `enableToonOptimization` | boolean | `false` | Enable/disable TOON optimization globally |

### Location of Configuration File

The `server_config.json` file can be in:
1. Current working directory (takes precedence)
2. MCP-CLI package directory (fallback)

## Benefits and Use Cases

### Cost Reduction

TOON optimization provides immediate cost savings on LLM API usage:

**Example Cost Savings** (based on GPT-4 pricing of $0.03/1K input tokens):

| Conversation Length | JSON Tokens | TOON Tokens | Saved Tokens | Cost Saved |
|---------------------|-------------|-------------|--------------|------------|
| 5 turns | 1,250 | 875 | 375 | $0.01125 |
| 10 turns | 2,500 | 1,750 | 750 | $0.02250 |
| 50 turns | 12,500 | 8,750 | 3,750 | $0.11250 |
| 100 turns | 25,000 | 17,500 | 7,500 | $0.22500 |

For high-volume usage, these savings compound significantly:
- **1,000 conversations/day** (50 turns each): ~$112.50/day saved = **$3,375/month**
- **10,000 conversations/day**: ~$1,125/day saved = **$33,750/month**

### Use Cases

#### 1. Long-Running Conversations
Perfect for applications with extended dialogue:
- Customer support chatbots
- Interactive tutoring systems
- Research assistants
- Code review sessions

**Benefit**: Token savings increase linearly with conversation length. A 100-turn conversation can save thousands of tokens.

#### 2. Tool-Heavy Applications
Ideal for systems using many MCP tools:
- Multi-tool workflows
- Complex automation systems
- API integrations with extensive function catalogs

**Benefit**: Tool definitions consume significant tokens. TOON compresses these definitions effectively.

#### 3. High-Volume Production Systems
Critical for production deployments:
- Enterprise chatbots serving thousands of users
- Automated customer service platforms
- Large-scale content generation systems

**Benefit**: Even 20-30% token savings translate to substantial cost reductions at scale.

#### 4. Development and Testing
Useful during development:
- Rapid prototyping with multiple iterations
- Testing conversation flows
- Debugging tool interactions

**Benefit**: Reduces development costs while maintaining full functionality.

#### 5. Resource-Constrained Environments
Valuable when token budgets are limited:
- Free-tier API usage
- Rate-limited applications
- Educational projects with budget constraints

**Benefit**: Maximizes the number of interactions possible within token limits.

## Technical Details

### Token Counting

TOON uses HuggingFace transformers library for accurate token counting with provider-specific tokenizers:

#### Tokenizer Mapping

```python
MODEL_TOKENIZER_MAP = {
    # OpenAI models
    "gpt-4": "Xenova/gpt-4",
    "gpt-4o": "Xenova/gpt-4o",
    "gpt-4o-mini": "Xenova/gpt-4o",
    "gpt-3.5-turbo": "Xenova/gpt-3.5-turbo",
    "o1": "Xenova/gpt-4o",
    "o3": "Xenova/gpt-4o",

    # Anthropic Claude models
    "claude": "Xenova/claude-tokenizer",

    # Meta Llama models
    "llama-3.1": "meta-llama/Llama-3.1-8B",
    "llama-3.2": "meta-llama/Llama-3.2-1B",

    # Mistral models
    "mistral": "mistralai/Mistral-7B-v0.1",

    # Google Gemini
    "gemini": "google/gemma-2b",

    # Groq (uses Llama)
    "groq": "meta-llama/Llama-2-7b-hf",
}
```

### Compression Strategies

TOON applies multiple compression techniques:

#### 1. Whitespace Removal
```python
# Before
{
  "role": "user",
  "content": "Hello"
}

# After
{"role":"user","content":"Hello"}
```

#### 2. JSON Compaction
```python
# Before
{
  "temperature": 72,
  "condition": "sunny",
  "humidity": 45
}

# After
{"temperature":72,"condition":"sunny","humidity":45}
```

#### 3. Content Optimization
- Removes extra spaces in text content
- Compacts multi-line content to single lines
- Preserves semantic meaning while reducing character count

#### 4. Structural Optimization
- Uses compact JSON serialization (`separators=(',', ':')`)
- Removes unnecessary Unicode escaping (`ensure_ascii=False`)
- Optimizes nested object representation

### Implementation Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   User Message Input                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Conversation Processor                      â”‚
â”‚  â€¢ Builds conversation history                          â”‚
â”‚  â€¢ Prepares messages for API                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               TOON Optimizer                             â”‚
â”‚  1. Convert messages to JSON format                     â”‚
â”‚  2. Convert messages to TOON format                     â”‚
â”‚  3. Count tokens for both (using transformers)          â”‚
â”‚  4. Compare and choose cheaper format                   â”‚
â”‚  5. Apply compression if beneficial                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Token Display (UI Feedback)                   â”‚
â”‚  ðŸ“Š Tokens: JSON=500 | TOON=350 | Saved=150 (30.0%)    â”‚
â”‚  âœ“ Using TOON compression to reduce tokens             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 LLM API Request                          â”‚
â”‚  â€¢ Sends compressed message payload                     â”‚
â”‚  â€¢ Receives standard LLM response                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Response Processing                       â”‚
â”‚  â€¢ Tool execution (if needed)                           â”‚
â”‚  â€¢ Next conversation turn                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Decision Algorithm

```python
def optimize_messages(messages, tools=None):
    if not self.enabled:
        return messages, stats

    # 1. Serialize to both formats
    json_str = json.dumps({"messages": messages}, ...)
    toon_str = self.convert_to_toon(messages, tools)

    # 2. Count tokens using provider-specific tokenizer
    json_tokens = self.count_tokens(json_str)
    toon_tokens = self.count_tokens(toon_str)

    # 3. Calculate savings
    saved_tokens = json_tokens - toon_tokens

    # 4. Make decision
    if saved_tokens > 0:
        return toon_messages, stats  # Use TOON
    else:
        return messages, stats       # Use JSON
```

## Token Savings Examples

### Example 1: Simple Q&A

**Scenario**: Single user question and response

**Without TOON** (JSON format):
```json
{
  "messages": [
    {
      "role": "user",
      "content": "What is the capital of France?"
    }
  ]
}
```
- **Tokens**: 25
- **Characters**: 89

**With TOON** (compressed):
```json
{"messages":[{"role":"user","content":"What is the capital of France?"}]}
```
- **Tokens**: 18
- **Characters**: 73
- **Saved**: 7 tokens (28%)

### Example 2: Multi-Turn Conversation

**Scenario**: 5 exchanges between user and assistant

**Without TOON**: 650 tokens
**With TOON**: 455 tokens
**Saved**: 195 tokens (30%)

### Example 3: Tool-Heavy Workflow

**Scenario**: User request with 5 available tools

**Without TOON**:
- Messages: 50 tokens
- Tool definitions: 500 tokens
- **Total**: 550 tokens

**With TOON**:
- Messages: 35 tokens
- Tool definitions: 350 tokens
- **Total**: 385 tokens
- **Saved**: 165 tokens (30%)

### Example 4: Extended Session

**Scenario**: 20-turn conversation with 3 tool calls

**Without TOON**: 5,200 tokens
**With TOON**: 3,640 tokens
**Saved**: 1,560 tokens (30%)

**Cost Impact** (at $0.03/1K tokens):
- Without TOON: $0.156
- With TOON: $0.109
- **Saved**: $0.047 per conversation

## Monitoring and Debugging

### Real-Time Display

When TOON optimization is enabled, you'll see token statistics for every API request:

```
ðŸ“Š Tokens: JSON=2,500 | TOON=1,750 | Saved=750 (30.0%)
âœ“ Using TOON compression to reduce tokens
```

Or if TOON doesn't provide savings:

```
ðŸ“Š Tokens: JSON=50 | TOON=52 | Saved=-2 (-4.0%)
Using JSON (no TOON savings for this request)
```

### API Request Logging

All API requests are logged to `~/.mcp-cli/api_requests.log` for debugging:

```
================================================================================
API REQUEST PAYLOAD [2025-01-15T10:30:45.123456]
================================================================================
Provider: openai
Model: gpt-4o
Number of messages: 3
Number of tools: 5
--------------------------------------------------------------------------------
Messages being sent to LLM:
[Message 0]
{"role":"user","content":"What is the weather?"}
[Message 1]
{"role":"assistant","content":"","tool_calls":[...]}
...
================================================================================
```

### Logging Configuration

Set log level to see detailed TOON optimization decisions:

```python
import logging
logging.getLogger('mcp_cli.llm.toon_optimizer').setLevel(logging.DEBUG)
```

Debug output includes:
- Tokenizer selection: `Loaded tokenizer 'Xenova/gpt-4o' for model 'gpt-4o'`
- Compression results: `JSON compression: 500 -> 350 chars (30.0% saved)`
- Token counts: `Token counts - JSON: 500, TOON: 350`
- Decisions: `Using TOON format: saved 150 tokens (30.0%)`

## Performance Considerations

### Overhead

TOON optimization adds minimal overhead:

1. **Token Counting**: ~1-5ms per request (using HuggingFace transformers)
2. **Compression**: ~0.5-2ms per request
3. **Total Overhead**: ~2-7ms per API call

This is negligible compared to typical LLM API latency (500-2000ms).

### Memory Usage

- **Tokenizer Model**: ~50-200MB loaded once at startup
- **Runtime Memory**: Minimal (<1MB per request)

### When TOON May Not Help

TOON optimization may provide minimal or no savings in these cases:

1. **Very Short Messages**: Single-word or very brief exchanges
   - Example: "Hi" â†’ No meaningful compression possible

2. **Already Compact Content**: Content without whitespace or structure
   - Example: Pre-compressed JSON or code

3. **First Message Only**: Initial conversation turn with no history
   - Savings increase with conversation length

### Best Practices

1. **Enable for Production**: Always enable for production deployments to maximize cost savings
2. **Monitor Logs**: Review API request logs periodically to verify savings
3. **Use with Long Conversations**: Maximum benefit in extended dialogues
4. **Combine with Tool Usage**: Tool-heavy workflows see significant savings
5. **Keep Enabled**: No downside to keeping it enabled - it only activates when beneficial

## Frequently Asked Questions

### Does TOON work with all LLM providers?

Yes! TOON optimization works with all supported providers:
- OpenAI (GPT-4, GPT-4o, GPT-3.5-turbo, O1, O3)
- Anthropic (Claude)
- Google (Gemini)
- Meta (Llama)
- Mistral
- Groq
- Ollama
- Any other provider supported by MCP-CLI

### Does TOON affect response quality?

No. TOON only compresses the request payload sent to the LLM. The LLM receives semantically identical information and produces the same quality responses.

### What if TOON makes the payload larger?

TOON automatically detects when compression would increase token count and uses the original JSON format instead. The decision is made intelligently for each request.

### Can I disable TOON temporarily?

Yes. Set `"enableToonOptimization": false` in your `server_config.json` and restart MCP-CLI.

### How accurate is the token counting?

Very accurate. TOON uses HuggingFace transformers with provider-specific tokenizers that match the actual tokenizers used by LLM providers.

### Does TOON require external dependencies?

Yes. TOON requires the `transformers` library for accurate token counting. Install with:
```bash
pip install transformers
```

If transformers is not available, TOON falls back to heuristic estimation (less accurate but functional).

## Related Documentation

- [Streaming Display](./STREAMING.md) - Real-time response visualization
- [Token Management](./TOKEN_MANAGEMENT.md) - OAuth token storage
- [Commands](./COMMANDS.md) - Available MCP-CLI commands

## Support and Contribution

For issues, feature requests, or contributions related to TOON optimization, please visit the MCP-CLI GitHub repository.
