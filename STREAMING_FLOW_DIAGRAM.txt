================================================================================
                    MCP-CLI STREAMING ARCHITECTURE FLOW
================================================================================

┌─────────────────────────────────────────────────────────────────────────────┐
│                           STREAMING INITIATION                              │
├─────────────────────────────────────────────────────────────────────────────┤
│
│  User Input (text message)
│         │
│         ├──> ConversationProcessor.process_conversation()
│         │
│         ├──> Check: Does client support streaming?
│         │    ├─ YES: _handle_streaming_completion()
│         │    └─ NO:  _handle_regular_completion()
│         │
│         └──> [STREAMING PATH] ──────────────────────────────────────────────┐
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                      STREAMING RESPONSE HANDLING                            │
├─────────────────────────────────────────────────────────────────────────────┤
│
│  _handle_streaming_completion()
│         │
│         ├──> ui_manager.start_streaming_response()  ───┐
│         │         │                                      │
│         │         └──> ChatDisplayManager.start_streaming()
│         │              ├─ Initialize display state
│         │              ├─ Start background refresh task (every 100ms)
│         │              └─ Activate live display
│         │
│         ├──> Create StreamingResponseHandler
│         │    └─ Pass: chat_display (ChatDisplayManager)
│         │
│         └──> handler.stream_response()  ─────────────────────────────────┐
│                                                                           │
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                    CHUNK PROCESSING LOOP (async)                            │
├─────────────────────────────────────────────────────────────────────────────┤
│
│  _handle_chuk_llm_streaming()
│         │
│         ├──> Loop: Get next chunk with 45s timeout per chunk
│         │    ├─ On timeout: Close stream, finalize, show warning
│         │    └─ On StopAsyncIteration: Stream ended normally
│         │
│         ├──> For each chunk:
│         │    │
│         │    ├─> _process_chunk(chunk, tool_calls)
│         │    │   │
│         │    │   ├─> Extract content
│         │    │   │   ├─ _extract_chunk_content()
│         │    │   │   │  └─ Detect: accumulated vs delta
│         │    │   │   │     └─ Append to current_response
│         │    │   │   │
│         │    │   │   ├─ Display update
│         │    │   │   │  └─ chat_display.update_streaming(content)
│         │    │   │   │
│         │    │   │   └─ Minimal sleep (0.5ms)
│         │    │   │
│         │    │   ├─> Extract tool calls
│         │    │   │   ├─ _extract_tool_calls_from_chunk()
│         │    │   │   └─ _process_tool_call_chunk()
│         │    │   │      └─ _accumulate_tool_call()
│         │    │   │
│         │    │   └─> Extract reasoning (DeepSeek)
│         │    │       └─ Update display with reasoning progress
│         │    │
│         │    └─> Check: User interrupted?
│         │        └─ YES: Close stream, break loop
│         │
│         └──> After stream ends: _finalize_streaming_tool_calls()
│                 └─ Validate, fix, clean tool calls
│
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                      RESPONSE FINALIZATION                                  │
├─────────────────────────────────────────────────────────────────────────────┤
│
│  _show_final_response()
│         │
│         ├──> Check: Has content and not already shown?
│         │
│         └──> ChatDisplayManager.finish_streaming()
│              ├─ Stop background refresh task
│              ├─ Calculate elapsed time
│              └─ Show final formatted response
│                 └─ Use CompactStreamingDisplay.get_final_panel()
│                    └─ Auto-render as Markdown if appropriate
│
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                     DECISION POINT: HAS TOOL CALLS?                         │
├─────────────────────────────────────────────────────────────────────────────┤
│
│  ConversationProcessor checks completion dict
│         │
│         ├─ NO tool_calls OR response only:
│         │  └──> Mark streaming stopped
│         │       └──> Add Message(ASSISTANT, content=response)
│         │           └──> RETURN TO PROMPT
│         │
│         └─ HAS tool_calls:
│            └──> Continue to Tool Execution Flow
│
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                    TOOL EXECUTION FLOW (Concurrent)                         │
├─────────────────────────────────────────────────────────────────────────────┤
│
│  ToolProcessor.process_tool_calls(tool_calls)
│         │
│         ├──> Add ONE assistant message with ALL tool_calls to history
│         │    └─ Message(ASSISTANT, tool_calls=[...])
│         │
│         ├──> For each tool_call CONCURRENTLY (max 4 at once):
│         │    │
│         │    ├──> _run_single_call(tool_call)
│         │    │    │
│         │    │    ├─> Extract tool name and arguments
│         │    │    │
│         │    │    ├─> Confirm execution (if configured)
│         │    │    │
│         │    │    ├─> ToolManager.execute_tool()
│         │    │    │   └─ (Skip loading indicator if streaming)
│         │    │    │
│         │    │    ├─> Format result
│         │    │    │
│         │    │    └─> Add TOOL message to history
│         │    │        └─ Message(TOOL, content=result)
│         │    │
│         │    └──> ui_manager.finish_tool_execution()
│         │
│         └──> Return control to ConversationProcessor
│              └──> Loop continues for next turn
│
└─────────────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────────────┐
│                      DISPLAY SYSTEM ARCHITECTURE                            │
├─────────────────────────────────────────────────────────────────────────────┤
│
│  PREFERRED PATH (ChatDisplayManager):
│  ──────────────────────────────────────
│  StreamingResponseHandler.stream_response()
│      └─> if self.chat_display:
│          ├─ chat_display.start_streaming()
│          ├─ [for each chunk]
│          │  └─ chat_display.update_streaming(content)
│          │     └─ [background task updates display every 100ms]
│          └─ chat_display.finish_streaming()
│             └─ Display final response
│
│  FALLBACK PATH (StreamingContext + CompactStreamingDisplay):
│  ────────────────────────────────────────────────────────────
│  StreamingResponseHandler.stream_response()
│      └─> elif not self.streaming_context:
│          ├─ Create StreamingContext (wraps rich.Live)
│          ├─ [for each chunk]
│          │  └─ streaming_context.update(content)
│          │     └─ Rich Live updates display each call
│          └─ streaming_context.__exit__()
│             └─ Show final panel
│
│  NOTE: Both paths use CompactStreamingDisplay for content detection
│        and final panel formatting
│
└─────────────────────────────────────────────────────────────────────────────┘

================================================================================
                             KEY FEATURES
================================================================================

TIMEOUTS:
  • Per-chunk: 45 seconds (handles DeepSeek Reasoner thinking)
  • Global: 300 seconds (safety limit)
  • Action: Close stream, finalize, show warning, return response

INTERRUPTS:
  • User can press Ctrl+C at any time
  • StreamingResponseHandler sets _interrupted flag
  • Closes stream iterator cleanly
  • Finalizes partial data before returning

TOOL CALL ACCUMULATION:
  • Detects when tool calls fragmented across chunks
  • Merges argument JSON intelligently
  • Validates completion (missing braces, etc.)
  • Finalizes AFTER stream completes

CONTENT TYPES:
  • Auto-detects: code, markdown, tables, JSON, SQL, HTML, text
  • Adapts display phase messages dynamically
  • Shows meaningful preview lines
  • Renders final response appropriately

REASONING CONTENT (DeepSeek):
  • Captures reasoning_content from streaming chunks
  • Shows reasoning progress in UI
  • Included in response dictionary
  • Added to conversation history

================================================================================
                         CRITICAL PATHS
================================================================================

1. TEXT RESPONSE (simple):
   Chunk 1 ──> Chunk 2 ──> ... ──> Stream ends ──> Display ──> Next turn

2. TOOL CALL RESPONSE:
   Chunk 1 ──> Tool data ──> ... ──> Finalize ──> Execute ──> Next turn

3. MIXED (text + tool calls):
   Content ──> Tool data ──> ... ──> Display + Execute ──> Next turn

4. INTERRUPTED:
   User Ctrl+C ──> Set flag ──> Close iterator ──> Finalize ──> Display

5. TIMEOUT:
   Waiting... (45s) ──> No chunk ──> Close ──> Finalize ──> Display

================================================================================
                      STATE TRACKING
================================================================================

StreamingResponseHandler state:
  ├─ current_response: str (accumulated content)
  ├─ _accumulated_tool_calls: list (tool calls being built)
  ├─ _previous_response_field: str (for accumulated detection)
  ├─ _reasoning_content: str (DeepSeek reasoning)
  ├─ _interrupted: bool (user cancel flag)
  ├─ _response_complete: bool (finalization flag)
  └─ chunk_count: int (debug info)

ChatDisplayManager state:
  ├─ is_streaming: bool
  ├─ streaming_content: str
  ├─ streaming_start_time: float
  ├─ reasoning_content: str
  ├─ chunks_received: int
  └─ _refresh_task: asyncio.Task

================================================================================
